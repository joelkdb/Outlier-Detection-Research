{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83e82b5",
   "metadata": {},
   "source": [
    "# Outlier Detection Benchmarks (OptiSLang-like) — Python Notebook\n",
    "\n",
    "This notebook provides a reusable API to:\n",
    "- Generate **Designs of Experiments (DoE)** for classic benchmarks (Branin, Rosenbrock, Hartmann, Kursawe)\n",
    "- **Inject outliers** in input space and/or response space\n",
    "- Run multiple **outlier detection** methods (IQR, Isolation Forest, LOF, One-Class SVM, Robust Covariance)\n",
    "- Fit a **Gaussian Process** surrogate and use its **predictive variance** to estimate a **pre-simulation outlier risk**\n",
    "- Visualize: scatter (2D), residual vs predicted (±σ bounds), IsolationForest score histogram, and GP variance contour (2D)\n",
    "\n",
    "> You can switch benchmarks by changing `BENCHMARK_NAME` in the demo cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6a352",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict, Tuple\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.metrics import r2_score\n",
    "from numpy.random import default_rng\n",
    "\n",
    "# For Mahalanobis distance p-values (chi-square), fallback if scipy unavailable\n",
    "try:\n",
    "    from scipy.stats import chi2\n",
    "    SCIPY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "rng = default_rng(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Benchmark:\n",
    "    name: str\n",
    "    dim: int\n",
    "    bounds: np.ndarray  # shape (dim, 2)\n",
    "    f: Callable[[np.ndarray], np.ndarray]  # expects (n, dim) -> (n,) or (n,k)\n",
    "\n",
    "def rosenbrock(x: np.ndarray, a: float = 1.0, b: float = 100.0) -> np.ndarray:\n",
    "    x = np.atleast_2d(x)\n",
    "    return np.sum(b * (x[:,1:] - x[:,:-1]**2)**2 + (a - x[:,:-1])**2, axis=1)\n",
    "\n",
    "def branin(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.atleast_2d(x)\n",
    "    x1, x2 = x[:,0], x[:,1]\n",
    "    a, b, c = 1.0, 5.1/(4*np.pi**2), 5/np.pi\n",
    "    r, s, t = 6.0, 10.0, 1/(8*np.pi)\n",
    "    return (a*(x2 - b*x1**2 + c*x1 - r)**2 + s*(1 - t)*np.cos(x1) + s)\n",
    "\n",
    "def hartmann3(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.atleast_2d(x)\n",
    "    alpha = np.array([1.0, 1.2, 3.0, 3.2])\n",
    "    A = np.array([[3.0, 10, 30],\n",
    "                  [0.1, 10, 35],\n",
    "                  [3.0, 10, 30],\n",
    "                  [0.1, 10, 35]])\n",
    "    P = 1e-4*np.array([[3689, 1170, 2673],\n",
    "                       [4699, 4387, 7470],\n",
    "                       [1091, 8732, 5547],\n",
    "                       [381,  5743, 8828]])\n",
    "    y = np.zeros(x.shape[0])\n",
    "    for i in range(4):\n",
    "        y -= alpha[i]*np.exp(-np.sum(A[i]*(x - P[i])**2, axis=1))\n",
    "    return y\n",
    "\n",
    "def kursawe_f1(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.atleast_2d(x)\n",
    "    s = np.sum(-10*np.exp(-0.2*np.sqrt(x[:,:-1]**2 + x[:,1:]**2)), axis=1)\n",
    "    return s\n",
    "\n",
    "BENCHMARKS: Dict[str, Benchmark] = {\n",
    "    \"rosenbrock2\": Benchmark(\"rosenbrock2\", 2, np.array([[-2.0, 2.0], [-1.0, 3.0]]), lambda X: rosenbrock(X)),\n",
    "    \"rosenbrock3\": Benchmark(\"rosenbrock3\", 3, np.array([[-2.0, 2.0], [-1.0, 3.0], [-2.0, 2.0]]), lambda X: rosenbrock(X)),\n",
    "    \"branin\":      Benchmark(\"branin\", 2, np.array([[-5.0, 10.0],[0.0, 15.0]]), branin),\n",
    "    \"hartmann3\":   Benchmark(\"hartmann3\", 3, np.array([[0.0, 1.0],[0.0, 1.0],[0.0, 1.0]]), hartmann3),\n",
    "    \"kursawe\":     Benchmark(\"kursawe\", 3, np.array([[-5.0, 5.0],[-5.0, 5.0],[-5.0, 5.0]]), kursawe_f1),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def sample_uniform(bounds: np.ndarray, n: int, rng=None) -> np.ndarray:\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    low, high = bounds[:,0], bounds[:,1]\n",
    "    u = rng.random((n, bounds.shape[0]))\n",
    "    return low + u*(high - low)\n",
    "\n",
    "def lhs(bounds: np.ndarray, n: int, rng=None) -> np.ndarray:\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    d = bounds.shape[0]\n",
    "    X = np.zeros((n, d))\n",
    "    for j in range(d):\n",
    "        perm = rng.permutation(n)\n",
    "        points = (perm + rng.random(n)) / n\n",
    "        low, high = bounds[j,0], bounds[j,1]\n",
    "        X[:, j] = low + points*(high - low)\n",
    "    return X\n",
    "\n",
    "def generate_dataset(bench, n: int, sampler: str=\"lhs\", rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    if sampler == \"lhs\":\n",
    "        X = lhs(bench.bounds, n, rng)\n",
    "    elif sampler == \"random\":\n",
    "        X = sample_uniform(bench.bounds, n, rng)\n",
    "    else:\n",
    "        raise ValueError(\"sampler must be 'lhs' or 'random'\")\n",
    "    y = bench.f(X)\n",
    "    return X, y\n",
    "\n",
    "def add_outliers(X: np.ndarray, y: np.ndarray, frac_x: float=0.0, frac_y: float=0.05,\n",
    "                 x_scale: float=3.0, y_scale: float=5.0, rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    n = X.shape[0]\n",
    "    mask = np.zeros(n, dtype=bool)\n",
    "\n",
    "    X_out = X.copy()\n",
    "    y_out = y.copy()\n",
    "\n",
    "    kx = int(np.round(frac_x * n))\n",
    "    if kx > 0:\n",
    "        idx_x = rng.choice(n, size=kx, replace=False)\n",
    "        mask[idx_x] = True\n",
    "        ranges = (X.max(axis=0) - X.min(axis=0))\n",
    "        shift = (rng.standard_normal((kx, X.shape[1])) * (x_scale * (ranges + 1e-9)))\n",
    "        X_out[idx_x] = X_out[idx_x] + shift\n",
    "\n",
    "    ky = int(np.round(frac_y * n))\n",
    "    if ky > 0:\n",
    "        idx_y = rng.choice(n, size=ky, replace=False)\n",
    "        mask[idx_y] = True\n",
    "        sigma = np.std(y) + 1e-9\n",
    "        y_out[idx_y] = y_out[idx_y] + rng.standard_normal(ky) * (y_scale * sigma)\n",
    "\n",
    "    return X_out, y_out, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "def detect_iqr(y: np.ndarray, k: float=1.5) -> np.ndarray:\n",
    "    q1, q3 = np.percentile(y, 25), np.percentile(y, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - k*iqr, q3 + k*iqr\n",
    "    return (y < lower) | (y > upper)\n",
    "\n",
    "def detect_isoforest(X: np.ndarray, contamination: float=0.05, random_state: int=42):\n",
    "    clf = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    pred = clf.fit_predict(X)\n",
    "    return pred == -1, clf\n",
    "\n",
    "def detect_lof(X: np.ndarray, contamination: float=0.05, n_neighbors: int=20):\n",
    "    lof = LocalOutlierFactor(contamination=contamination, n_neighbors=n_neighbors)\n",
    "    pred = lof.fit_predict(X)\n",
    "    return pred == -1, lof\n",
    "\n",
    "def detect_ocsvm(X: np.ndarray, nu: float=0.05, gamma=\"scale\"):\n",
    "    oc = OneClassSVM(nu=nu, kernel=\"rbf\", gamma=gamma)\n",
    "    pred = oc.fit_predict(X)\n",
    "    return pred == -1, oc\n",
    "\n",
    "def detect_robust_cov(X: np.ndarray, contamination: float=0.05):\n",
    "    ee = EllipticEnvelope(contamination=contamination, support_fraction=1.0, random_state=42)\n",
    "    pred = ee.fit_predict(X)\n",
    "    return pred == -1, ee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4c40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "try:\n",
    "    from scipy.stats import chi2\n",
    "    SCIPY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "def fit_gp(X: np.ndarray, y: np.ndarray) -> GaussianProcessRegressor:\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=np.ones(X.shape[1]), length_scale_bounds=(1e-2, 1e3))              + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-9, 1e-1))\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3, normalize_y=True, random_state=42)\n",
    "    gp.fit(X, y)\n",
    "    return gp\n",
    "\n",
    "def residual_diagnostics(gp: GaussianProcessRegressor, X: np.ndarray, y: np.ndarray):\n",
    "    y_pred, y_std = gp.predict(X, return_std=True)\n",
    "    resid = y - y_pred\n",
    "    sigma_resid = np.std(resid)\n",
    "    return y_pred, y_std, resid, sigma_resid\n",
    "\n",
    "def mahalanobis_pvals(X: np.ndarray):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    cov = np.cov(X, rowvar=False) + 1e-9*np.eye(X.shape[1])\n",
    "    inv = np.linalg.inv(cov)\n",
    "    dif = X - mu\n",
    "    d2 = np.einsum('ni,ij,nj->n', dif, inv, dif)\n",
    "    if SCIPY_AVAILABLE:\n",
    "        p = 1.0 - chi2.cdf(d2, df=X.shape[1])\n",
    "    else:\n",
    "        p = np.full_like(d2, np.nan, dtype=float)\n",
    "    return d2, p\n",
    "\n",
    "def risk_before_simulation(X_train: np.ndarray, gp: GaussianProcessRegressor=None, X_cand: np.ndarray=None):\n",
    "    X_eval = X_train if X_cand is None else X_cand\n",
    "    mu = np.mean(X_train, axis=0)\n",
    "    cov = np.cov(X_train, rowvar=False) + 1e-9*np.eye(X_train.shape[1])\n",
    "    inv = np.linalg.inv(cov)\n",
    "    dif = X_eval - mu\n",
    "    d2_eval = np.einsum('ni,ij,nj->n', dif, inv, dif)\n",
    "\n",
    "    risk = {\"mahalanobis_d2\": d2_eval}\n",
    "    if SCIPY_AVAILABLE:\n",
    "        risk[\"mahalanobis_p\"] = 1.0 - chi2.cdf(d2_eval, df=X_train.shape[1])\n",
    "    if gp is not None:\n",
    "        _, std = gp.predict(X_eval, return_std=True)\n",
    "        risk[\"gp_sigma\"] = std\n",
    "    return risk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a9602",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "def plot_scatter_2d(X: np.ndarray, y: np.ndarray, mask_outliers=None, title=\"2D scatter (color=y)\"):\n",
    "    if X.shape[1] != 2:\n",
    "        raise ValueError(\"plot_scatter_2d requires 2D inputs.\")\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sc = plt.scatter(X[:,0], X[:,1], s=30, c=y)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.title(title)\n",
    "    plt.colorbar(sc, label=\"y\")\n",
    "    if mask_outliers is not None:\n",
    "        idx = np.where(mask_outliers)[0]\n",
    "        plt.scatter(X[idx,0], X[idx,1], s=80, facecolors='none', edgecolors='k')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, sigma_resid, k=2.0, title=\"Residuals vs Predicted (±kσ)\"):\n",
    "    resid = y_true - y_pred\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(y_pred, resid, s=25)\n",
    "    plt.axhline(0.0)\n",
    "    plt.axhline(+k*sigma_resid, linestyle=\"--\")\n",
    "    plt.axhline(-k*sigma_resid, linestyle=\"--\")\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"Residual\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_isoforest_scores(clf: IsolationForest):\n",
    "    scores = clf.score_samples(clf._fit_X)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(scores, bins=30)\n",
    "    plt.xlabel(\"IsolationForest score\"); plt.ylabel(\"Count\")\n",
    "    plt.title(\"Distribution of IsolationForest scores\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_gp_variance_2d(gp: GaussianProcessRegressor, bounds: np.ndarray, grid_n: int=80, title=\"GP predictive std\"):\n",
    "    if bounds.shape[0] != 2:\n",
    "        raise ValueError(\"plot_gp_variance_2d requires 2D bounds.\")\n",
    "    x1 = np.linspace(bounds[0,0], bounds[0,1], grid_n)\n",
    "    x2 = np.linspace(bounds[1,0], bounds[1,1], grid_n)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    Xg = np.column_stack([X1.ravel(), X2.ravel()])\n",
    "    _, std = gp.predict(Xg, return_std=True)\n",
    "    Z = std.reshape(X1.shape)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    cs = plt.contourf(X1, X2, Z, levels=20)\n",
    "    plt.colorbar(cs, label=\"Pred. std (σ)\")\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.title(title)\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1bd264",
   "metadata": {},
   "source": [
    "## Demo: choose a benchmark, create DoE, inject outliers, run detectors, and visualize\n",
    "You can change `BENCHMARK_NAME`, sampling size `N`, and outlier fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569bf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === User parameters ===\n",
    "BENCHMARK_NAME = \"branin\"     # \"branin\", \"rosenbrock2\", \"rosenbrock3\", \"hartmann3\", \"kursawe\"\n",
    "N = 200\n",
    "SAMPLER = \"lhs\"               # \"lhs\" or \"random\"\n",
    "FRAC_X = 0.00                 # fraction of input-space outliers\n",
    "FRAC_Y = 0.05                 # fraction of response-space outliers\n",
    "\n",
    "bench = BENCHMARKS[BENCHMARK_NAME]\n",
    "X, y = generate_dataset(bench, N, SAMPLER)\n",
    "Xo, yo, mask_injected = add_outliers(X, y, frac_x=FRAC_X, frac_y=FRAC_Y)\n",
    "\n",
    "print(f\"Benchmark: {bench.name}  | dim={bench.dim}\")\n",
    "print(f\"Bounds:\\n{bench.bounds}\")\n",
    "print(f\"Injected outliers: {mask_injected.sum()} / {N}\")\n",
    "\n",
    "# Visualize if 2D\n",
    "if bench.dim == 2:\n",
    "    plot_scatter_2d(Xo, yo, mask_outliers=mask_injected, title=f\"{bench.name}: DoE with injected outliers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Detectors ---\n",
    "mask_iqr = detect_iqr(yo, k=1.5)\n",
    "mask_iso, clf_iso = detect_isoforest(Xo, contamination=max(0.01, FRAC_Y + FRAC_X))\n",
    "mask_lof, lof = detect_lof(Xo, contamination=max(0.01, FRAC_Y + FRAC_X))\n",
    "mask_oc, oc = detect_ocsvm(Xo, nu=max(0.01, FRAC_Y + FRAC_X))\n",
    "mask_rc, ee = detect_robust_cov(Xo, contamination=max(0.01, FRAC_Y + FRAC_X))\n",
    "\n",
    "def report(name, mask):\n",
    "    tp = np.sum(mask & mask_injected)\n",
    "    fp = np.sum(mask & ~mask_injected)\n",
    "    fn = np.sum(~mask & mask_injected)\n",
    "    print(f\"{name:18s}  flagged={mask.sum():3d} | TP={tp:3d}  FP={fp:3d}  FN={fn:3d}\")\n",
    "\n",
    "print(\"\\nDetection summary (vs injected ground-truth mask):\")\n",
    "report(\"IQR(y)\", mask_iqr)\n",
    "report(\"IsolationForest\", mask_iso)\n",
    "report(\"LOF\", mask_lof)\n",
    "report(\"One-Class SVM\", mask_oc)\n",
    "report(\"Robust Covariance\", mask_rc)\n",
    "\n",
    "# Residual diagnostics via GP\n",
    "gp = fit_gp(Xo[~mask_iso], yo[~mask_iso])  # fit on points not flagged by IF (simple robustification)\n",
    "y_pred, y_std, resid, sigma_resid = residual_diagnostics(gp, Xo, yo)\n",
    "from sklearn.metrics import r2_score\n",
    "print(f\"\\nGP R^2 on all points: {r2_score(yo, y_pred):.3f}, residual σ={sigma_resid:.3g}\")\n",
    "\n",
    "# Plots\n",
    "plot_residuals(yo, y_pred, sigma_resid, k=2.0, title=\"Residuals vs Predicted (±2σ)\")\n",
    "plot_isoforest_scores(clf_iso)\n",
    "\n",
    "if bench.dim == 2:\n",
    "    plot_gp_variance_2d(gp, bench.bounds, grid_n=80, title=\"GP predictive std (pre-simulation risk proxy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1faca7",
   "metadata": {},
   "source": [
    "## Pre-simulation outlier risk for candidate designs\n",
    "- **Mahalanobis distance** in input space (p-value if SciPy available)\n",
    "- **GP predictive std** (if GP available) as uncertainty-driven risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f155e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a small candidate set (e.g., random within bounds)\n",
    "m = 10\n",
    "X_cand = sample_uniform(bench.bounds, m)\n",
    "\n",
    "risk = risk_before_simulation(Xo[~mask_iso], gp=gp, X_cand=X_cand)\n",
    "print(\"Risk keys:\", list(risk.keys()))\n",
    "print(\"Top-5 risky by Mahalanobis d^2:\")\n",
    "top_idx = np.argsort(-risk[\"mahalanobis_d2\"])[:5]\n",
    "for i in top_idx:\n",
    "    msg = f\"idx={i:2d}  d2={risk['mahalanobis_d2'][i]:8.3f}\"\n",
    "    if 'mahalanobis_p' in risk:\n",
    "        msg += f\"  p={risk['mahalanobis_p'][i]:.3e}\"\n",
    "    if 'gp_sigma' in risk:\n",
    "        msg += f\"  gpσ={risk['gp_sigma'][i]:.3f}\"\n",
    "    print(msg)\n",
    "\n",
    "# Optional plot if 2D: overlay GP std as contours with candidate points (separate single plot)\n",
    "if bench.dim == 2:\n",
    "    plot_gp_variance_2d(gp, bench.bounds, grid_n=80, title=\"GP σ (pre-sim risk)\")\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X_cand[:,0], X_cand[:,1], s=60)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\"); plt.title(\"Candidate designs (for risk eval)\")\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
