{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c30839",
   "metadata": {},
   "source": [
    "# Outlier Detection Benchmarks (Simplified, pymoo-based)\n",
    "\n",
    "Uses `pymoo` to define benchmarks (Branin, Rosenbrock, Hartmann3, Kursawe).\n",
    "- DoE (random or LHS)\n",
    "- Outlier injection on y\n",
    "- Detectors: IQR(y), IsolationForest, LOF, One-Class SVM, Robust Covariance\n",
    "- Gaussian Process residual diagnostics and pre-simulation risk\n",
    "\n",
    "Multi-objective problems are reduced to `y = F[:, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9216a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "from pymoo.problems import get_problem\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "try:\n",
    "    from scipy.stats import chi2\n",
    "    SCIPY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SCIPY_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c93d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lhs(bounds: np.ndarray, n: int, rng=None) -> np.ndarray:\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    d = bounds.shape[0]\n",
    "    X = np.zeros((n, d))\n",
    "    for j in range(d):\n",
    "        perm = rng.permutation(n)\n",
    "        pts = (perm + rng.random(n)) / n\n",
    "        low, high = bounds[j, 0], bounds[j, 1]\n",
    "        X[:, j] = low + pts * (high - low)\n",
    "    return X\n",
    "\n",
    "def sample_uniform(bounds: np.ndarray, n: int, rng=None) -> np.ndarray:\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    low, high = bounds[:, 0], bounds[:, 1]\n",
    "    u = rng.random((n, bounds.shape[0]))\n",
    "    return low + u * (high - low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fe5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds_from_problem(problem) -> np.ndarray:\n",
    "    return np.column_stack([problem.xl, problem.xu])\n",
    "\n",
    "def generate_dataset(problem_name: str, n: int, sampler: str = \"lhs\", rng=None) -> Tuple[np.ndarray, np.ndarray, object]:\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    problem = get_problem(problem_name)\n",
    "    bounds = get_bounds_from_problem(problem)\n",
    "    if sampler == \"lhs\":\n",
    "        X = lhs(bounds, n, rng)\n",
    "    elif sampler == \"random\":\n",
    "        X = sample_uniform(bounds, n, rng)\n",
    "    else:\n",
    "        raise ValueError(\"sampler must be 'lhs' or 'random'\")\n",
    "    F = problem.evaluate(X)\n",
    "    y = F[:, 0] if F.ndim == 2 else F\n",
    "    y = np.asarray(y).ravel()\n",
    "    return X, y, problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d56ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_outliers_y(y: np.ndarray, frac_y: float = 0.05, scale: float = 5.0, rng=None):\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    n = y.shape[0]\n",
    "    mask = np.zeros(n, dtype=bool)\n",
    "    k = int(np.round(frac_y * n))\n",
    "    y_out = y.copy()\n",
    "    if k > 0:\n",
    "        idx = rng.choice(n, size=k, replace=False)\n",
    "        mask[idx] = True\n",
    "        sigma = np.std(y) + 1e-9\n",
    "        y_out[idx] = y_out[idx] + rng.standard_normal(k) * (scale * sigma)\n",
    "    return y_out, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_iqr(y: np.ndarray, k: float = 1.5) -> np.ndarray:\n",
    "    q1, q3 = np.percentile(y, 25), np.percentile(y, 75)\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - k * iqr, q3 + k * iqr\n",
    "    return (y < lower) | (y > upper)\n",
    "\n",
    "def detect_isoforest(X: np.ndarray, contamination: float = 0.05, random_state: int = 42):\n",
    "    clf = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    pred = clf.fit_predict(X)\n",
    "    return pred == -1, clf\n",
    "\n",
    "def detect_lof(X: np.ndarray, contamination: float = 0.05, n_neighbors: int = 20):\n",
    "    lof = LocalOutlierFactor(contamination=contamination, n_neighbors=n_neighbors)\n",
    "    pred = lof.fit_predict(X)\n",
    "    return pred == -1, lof\n",
    "\n",
    "def detect_ocsvm(X: np.ndarray, nu: float = 0.05, gamma = \"scale\"):\n",
    "    oc = OneClassSVM(nu=nu, kernel=\"rbf\", gamma=gamma)\n",
    "    pred = oc.fit_predict(X)\n",
    "    return pred == -1, oc\n",
    "\n",
    "def detect_robust_cov(X: np.ndarray, contamination: float = 0.05):\n",
    "    ee = EllipticEnvelope(contamination=contamination, support_fraction=1.0, random_state=42)\n",
    "    pred = ee.fit_predict(X)\n",
    "    return pred == -1, ee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15467fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gp(X: np.ndarray, y: np.ndarray) -> GaussianProcessRegressor:\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=np.ones(X.shape[1]), length_scale_bounds=(1e-2, 1e3)) \\\n",
    "             + WhiteKernel(noise_level=1e-3, noise_level_bounds=(1e-9, 1e-1))\n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=3, normalize_y=True, random_state=42)\n",
    "    gp.fit(X, y)\n",
    "    return gp\n",
    "\n",
    "def residual_diagnostics(gp: GaussianProcessRegressor, X: np.ndarray, y: np.ndarray):\n",
    "    y_pred, y_std = gp.predict(X, return_std=True)\n",
    "    resid = y - y_pred\n",
    "    sigma_resid = np.std(resid)\n",
    "    return y_pred, y_std, resid, sigma_resid\n",
    "\n",
    "def mahalanobis_d2(X: np.ndarray):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    cov = np.cov(X, rowvar=False) + 1e-9 * np.eye(X.shape[1])\n",
    "    inv = np.linalg.inv(cov)\n",
    "    dif = X - mu\n",
    "    d2 = np.einsum('ni,ij,nj', dif, inv, dif)\n",
    "    return d2\n",
    "\n",
    "def risk_before_simulation(X_train: np.ndarray, X_eval: np.ndarray, gp: GaussianProcessRegressor = None):\n",
    "    d2 = mahalanobis_d2(np.vstack([X_train, X_eval]))[-X_eval.shape[0]:]\n",
    "    risk = { 'mahalanobis_d2': d2 }\n",
    "    if SCIPY_AVAILABLE:\n",
    "        risk['mahalanobis_p'] = 1.0 - chi2.cdf(d2, df=X_train.shape[1])\n",
    "    if gp is not None:\n",
    "        _, std = gp.predict(X_eval, return_std=True)\n",
    "        risk['gp_sigma'] = std\n",
    "    return risk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f442b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_2d(X: np.ndarray, y: np.ndarray, mask_outliers=None, title='2D scatter (color=y)'):\n",
    "    if X.shape[1] != 2:\n",
    "        raise ValueError('plot_scatter_2d requires 2D inputs.')\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sc = plt.scatter(X[:, 0], X[:, 1], s=30, c=y)\n",
    "    plt.xlabel('x1'); plt.ylabel('x2'); plt.title(title)\n",
    "    plt.colorbar(sc, label='y')\n",
    "    if mask_outliers is not None:\n",
    "        idx = np.where(mask_outliers)[0]\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], s=80, facecolors='none', edgecolors='k')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_residuals(y_true, y_pred, sigma_resid, k=2.0, title='Residuals vs Predicted (±kσ)'):\n",
    "    resid = y_true - y_pred\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(y_pred, resid, s=25)\n",
    "    plt.axhline(0.0)\n",
    "    plt.axhline(+k * sigma_resid, linestyle='--')\n",
    "    plt.axhline(-k * sigma_resid, linestyle='--')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('Residual')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_isoforest_scores(clf: IsolationForest, X_fit: np.ndarray):\n",
    "    # FIX: pass the training or evaluation X explicitly; do not use private attributes\n",
    "    scores = clf.score_samples(X_fit)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(scores, bins=30)\n",
    "    plt.xlabel('IsolationForest score'); plt.ylabel('Count')\n",
    "    plt.title('Distribution of IsolationForest scores')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_gp_variance_2d(gp: GaussianProcessRegressor, bounds: np.ndarray, grid_n: int = 80, title='GP predictive std'):\n",
    "    if bounds.shape[0] != 2:\n",
    "        raise ValueError('plot_gp_variance_2d requires 2D bounds.')\n",
    "    x1 = np.linspace(bounds[0, 0], bounds[0, 1], grid_n)\n",
    "    x2 = np.linspace(bounds[1, 0], bounds[1, 1], grid_n)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    Xg = np.column_stack([X1.ravel(), X2.ravel()])\n",
    "    _, std = gp.predict(Xg, return_std=True)\n",
    "    Z = std.reshape(X1.shape)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    cs = plt.contourf(X1, X2, Z, levels=20)\n",
    "    plt.colorbar(cs, label='Pred. std (σ)')\n",
    "    plt.xlabel('x1'); plt.ylabel('x2'); plt.title(title)\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e872191",
   "metadata": {},
   "source": [
    "## Demo\n",
    "Set `PROBLEM_NAME`, `N`, `SAMPLER`, and `FRAC_Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "PROBLEM_NAME = 'kursawe'   # 'branin' | 'rosenbrock' | 'hartmann3' | 'kursawe'\n",
    "N = 200\n",
    "SAMPLER = 'lhs'            # 'lhs' or 'random'\n",
    "FRAC_Y = 0.05              # fraction of response-space outliers\n",
    "\n",
    "# 1) Generate dataset\n",
    "X, y, problem = generate_dataset(PROBLEM_NAME, N, SAMPLER)\n",
    "\n",
    "# 2) Inject outliers (response only)\n",
    "y_noisy, mask_injected = add_outliers_y(y, frac_y=FRAC_Y, scale=5.0)\n",
    "\n",
    "# 3) Run detectors\n",
    "mask_iqr = detect_iqr(y_noisy, k=1.5)\n",
    "mask_iso, clf_iso = detect_isoforest(X, contamination=max(0.01, FRAC_Y))\n",
    "mask_lof, lof = detect_lof(X, contamination=max(0.01, FRAC_Y))\n",
    "mask_oc, oc = detect_ocsvm(X, nu=max(0.01, FRAC_Y))\n",
    "mask_rc, ee = detect_robust_cov(X, contamination=max(0.01, FRAC_Y))\n",
    "\n",
    "def report(name, mask):\n",
    "    tp = np.sum(mask & mask_injected)\n",
    "    fp = np.sum(mask & ~mask_injected)\n",
    "    fn = np.sum(~mask & mask_injected)\n",
    "    print(f\"{name:18s}  flagged={mask.sum():3d} | TP={tp:3d}  FP={fp:3d}  FN={fn:3d}\")\n",
    "\n",
    "print('Detection summary (vs injected ground truth):')\n",
    "report('IQR(y)', mask_iqr)\n",
    "report('IsolationForest', mask_iso)\n",
    "report('LOF', mask_lof)\n",
    "report('One-Class SVM', mask_oc)\n",
    "report('Robust Covariance', mask_rc)\n",
    "\n",
    "# 4) GP residual diagnostics (fit on IF-inliers)\n",
    "gp = fit_gp(X[~mask_iso], y_noisy[~mask_iso])\n",
    "y_pred, y_std, resid, sigma_resid = residual_diagnostics(gp, X, y_noisy)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print(f\"GP R^2 on all points: {r2_score(y_noisy, y_pred):.3f}, residual σ={sigma_resid:.3g}\")\n",
    "\n",
    "# 5) Plots (each in its own figure)\n",
    "if X.shape[1] == 2:\n",
    "    bounds = np.column_stack([problem.xl, problem.xu])\n",
    "    plot_scatter_2d(X, y_noisy, mask_outliers=mask_injected, title=f\"{PROBLEM_NAME}: DoE with injected outliers\")\n",
    "    plot_gp_variance_2d(gp, bounds, grid_n=80, title=\"GP predictive std (pre-sim risk proxy)\")\n",
    "\n",
    "plot_residuals(y_noisy, y_pred, sigma_resid, k=2.0, title='Residuals vs Predicted (±2σ)')\n",
    "plot_isoforest_scores(clf_iso, X_fit=X)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
